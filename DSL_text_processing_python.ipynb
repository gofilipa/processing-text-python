{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czh_fgjWpusv"
      },
      "source": [
        "# Text Processing with Python\n",
        "The goal of this workshop is to process, annotate, and analyze text in order to enable\n",
        "analysis and insights.\n",
        "\n",
        "To do so, we will be working with the Python programming langauge, and in particular, a package called [spaCy](https://spacy.io/), which was designed to work with text-based data. With spaCy, which is the industry standard for text-based analysis, we will extract patterns based on linguistic elements.\n",
        "\n",
        "The dataset we will use comes from from a reality TV show, Love Is Blind, which is a dating show. To get the dataset, I scraped transcripts of the show and saved them to a .json file, which we will download through a URL. From this dataset, we will ask questions about the basic plot of the show, which is a romantic plot between several characters. We will also explore less obvious elements from the narrative, using spaCy to surface things we may not have noticed before.\n",
        "\n",
        "Let's jump in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7Gf4yvbp4a3"
      },
      "source": [
        "## loading our libraries and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAlRoLnsTTVF"
      },
      "outputs": [],
      "source": [
        "# importing our spacy and pandas libraries\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# loading up the model in english\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLC06XFcTrnN"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(\"https://bit.ly/lib_transcripts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZ2gs9JSVngi"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mowLHJK_VsBr"
      },
      "outputs": [],
      "source": [
        "# displaying columns\n",
        "\n",
        "df['transcript_text'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkNCnI_nV1nj"
      },
      "outputs": [],
      "source": [
        "df['transcript_text'][1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjtLosKzXPdF"
      },
      "source": [
        "## exploring linguistic annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mA06mFdOXfi7"
      },
      "outputs": [],
      "source": [
        "doc = nlp(df['transcript_text'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "den_GLCBX2_B"
      },
      "outputs": [],
      "source": [
        "## commenting out this line because output is too long\n",
        "\n",
        "# doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGSrDYUK74ZT"
      },
      "source": [
        "We will use list slicing to explore sections (slices) of our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqFnICfFX3Cm"
      },
      "outputs": [],
      "source": [
        "# how do we create slices? let's see the first 10 words in our doc.\n",
        "\n",
        "doc[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FooUO371X3Gh"
      },
      "outputs": [],
      "source": [
        "# practice creating slices of random bits of text\n",
        "\n",
        "doc[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gnBVTLlY6VU"
      },
      "outputs": [],
      "source": [
        "doc[300:350]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IAbdkSVZFub"
      },
      "outputs": [],
      "source": [
        "doc[-100:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lq6uf1RY6bI"
      },
      "outputs": [],
      "source": [
        "len(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ougeOfQI7-Pe"
      },
      "source": [
        "We use loops to iterate through our dataset and pull out individual words. In this case, we are pulling out not only the word, called `token`, but also its attributes. Read more about tokens and their attributes on the [spaCy docs](https://spacy.io/api/token#attributes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WcX1yLtZBlj"
      },
      "outputs": [],
      "source": [
        "# writing loops to pull out data\n",
        "\n",
        "for i in doc[:5]:\n",
        "  print(i.text)\n",
        "  print(i.lemma_) # why would you want to pull out the lemma?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXjei2CraFs2"
      },
      "outputs": [],
      "source": [
        "# part of speech\n",
        "\n",
        "for i in doc[:5]:\n",
        "  print(i.text, i.pos_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvG8DxRFZBr2"
      },
      "outputs": [],
      "source": [
        "# other useful properties\n",
        "\n",
        "for i in doc[:5]:\n",
        "  print(i.text, i.pos_, i.shape_, i.is_alpha, i.is_stop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjfJuBP1cZmw"
      },
      "outputs": [],
      "source": [
        "# filtering:\n",
        "# using conditional statement to print out nouns\n",
        "\n",
        "for i in doc[:100]:\n",
        "  if i.pos_ == \"NOUN\":\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDDQLdH3rYzL"
      },
      "outputs": [],
      "source": [
        "# filtering:\n",
        "# using conditional statement to print out verbs\n",
        "\n",
        "for i in doc[:100]:\n",
        "  if i.pos_ == \"VERB\":\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqVpY_DBZByZ"
      },
      "outputs": [],
      "source": [
        "# see the docs object is quite complex, filled with annotations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3mKrQP5oG-7"
      },
      "source": [
        "## pulling out phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q29hR_-iFxg9"
      },
      "outputs": [],
      "source": [
        "# noun chunks with spacy\n",
        "\n",
        "for chunk in doc[:20].noun_chunks:\n",
        "  print(chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEYYq6vdZB__"
      },
      "outputs": [],
      "source": [
        "for i in doc[:100].sents:\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jEDDErprqSB"
      },
      "source": [
        "## pulling out entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSlt9c40I4iB"
      },
      "outputs": [],
      "source": [
        "doc[:1000].ents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blV_D8UKI0NU"
      },
      "outputs": [],
      "source": [
        "# pull out entities\n",
        "\n",
        "for i in doc[:1000].ents:\n",
        "  print(i, i.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRd448jwK9RP"
      },
      "source": [
        "## expanding from doc to docs\n",
        "Now we will expand to all 13 episodes from the second season of the show. Then, we will be able to explore what happens episode by episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3r-DL5GgZCHM"
      },
      "outputs": [],
      "source": [
        "# we are going to make a new doc based on just the second season of the show.\n",
        "\n",
        "docs = list(nlp.pipe(df['transcript_text'][0:13]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DcEfBrjXxCe"
      },
      "outputs": [],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ui-ylUW0zs4u"
      },
      "source": [
        "The syntax in our loop is going to change. Now we need to use list indexing to indicate the *episode* we want to loop through in square brackets. After that, we can indicate the slice of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y05SUsUAIjlS"
      },
      "outputs": [],
      "source": [
        "# pull out entities for the first 200 words [:200] of the last episode [-1]\n",
        "\n",
        "for i in docs[-1][:200].ents:\n",
        "  print(i, i.label_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKjlp3yisAlf"
      },
      "outputs": [],
      "source": [
        "# pulling out entities for the first 200 words in the first episode\n",
        "\n",
        "for i in docs[0][:200].ents:\n",
        "  print(i, i.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxsDrC8RNRIs"
      },
      "source": [
        "Another way of doing this is to add another level to our loop. This way, we can loop through all of the episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG5TDPtHNUqp"
      },
      "outputs": [],
      "source": [
        "for episode in docs:\n",
        "  for word in episode.ents[:3]: # slicing it at 3 words into each episode, to avoid being too long in the notebook\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_AuS-sLLTSV"
      },
      "outputs": [],
      "source": [
        "# how would I pull out entities for just characters?\n",
        "\n",
        "for episode in docs:\n",
        "  for word in episode.ents[:3]:\n",
        "    if word.label_ == \"PERSON\":\n",
        "      print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPBbAcDIJPJC"
      },
      "source": [
        "## who are the couples? in the first 3 episodes\n",
        "Try to determine the identities of the couples that are dating each other on the show.\n",
        "\n",
        "How might you go about doing that, using spaCy's annotation capabilities? What linguistic elements would help you to assess whether or not a character is a main character?\n",
        "\n",
        "Go through the first 3 episodes, and determine who is dating whom in that episode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzShtaKMBPIC"
      },
      "outputs": [],
      "source": [
        "for word in docs[0].ents:\n",
        "  if word.label_ == \"PERSON\":\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VdFhA1eHq8v"
      },
      "source": [
        "## what happens to the couples?\n",
        "\n",
        "Now for the big question: what happens to the couples in the show? Do they stay together? Why or why not?\n",
        "\n",
        "We will use spacy's linguistic annotations to write a conditional statement that pulls out sentences relevant to our inquiry. We want a sentence like \"person loves person\", or \"person married person\", or \"person dumped person\".\n",
        "\n",
        "First, we create a list of romantic verbs, which we will use to filter sentences. For that, we can re-deploy our conditional statement above that checks for verbs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRSX8-Rs1Vn-"
      },
      "outputs": [],
      "source": [
        "for episode in docs:\n",
        "  for i in episode[:10]:\n",
        "    if i.pos_ == 'VERB':\n",
        "      print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-PrKGtz1169"
      },
      "source": [
        "Then we write out the condition for filtering, which checks for a part of speech and if the verb is in our list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ex8GPTv28uZ"
      },
      "outputs": [],
      "source": [
        "romantic_verbs = [\"love\", \"marry\", \"kiss\", \"propose\", \"date\", \"betray\"]\n",
        "\n",
        "for i in docs[0].sents:\n",
        "  for token in i:\n",
        "    if token.lemma_.lower() in romantic_verbs and token.pos_ == \"VERB\":\n",
        "      print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0be1sfs4N0w"
      },
      "source": [
        "Now we will add a condition that checks if there's a PER entity in our sentence. Our loop looks a little unwieldy, but it works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcCiY3Hj3WAf"
      },
      "outputs": [],
      "source": [
        "romantic_verbs = {\"love\", \"marry\", \"kiss\", \"propose\", \"date\", \"betray\"}\n",
        "\n",
        "for i in docs[-1].sents:\n",
        "  for token in i:\n",
        "    if token.lemma_.lower() in romantic_verbs and token.pos_ == \"VERB\":\n",
        "      for ent in i.ents:\n",
        "        if ent.label_ == \"PERSON\":\n",
        "          print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_WuVVV17iI_"
      },
      "source": [
        "Spend some time exploring the dataset, episode by episode, and try to figure out what happens with the couples."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_3mKrQP5oG-7",
        "0jEDDErprqSB",
        "JRd448jwK9RP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
